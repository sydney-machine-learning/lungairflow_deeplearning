{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyDOE==0.3.7\n!git clone https://github.com/maziarraissi/PINNs.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T13:42:29.129743Z","iopub.execute_input":"2021-12-16T13:42:29.130264Z","iopub.status.idle":"2021-12-16T13:43:08.126050Z","shell.execute_reply.started":"2021-12-16T13:42:29.130168Z","shell.execute_reply":"2021-12-16T13:43:08.125093Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\n#mpl.use('pgf')\n\ndef figsize(scale, nplots = 1):\n    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n    fig_height = nplots*fig_width*golden_mean       # height in inches\n    fig_size = [fig_width,fig_height]\n    return fig_size\n\nimport matplotlib.pyplot as plt\n\n# I make my own newfig and savefig functions\ndef newfig(width, nplots = 1):\n    fig = plt.figure(figsize=figsize(width, nplots))\n    ax = fig.add_subplot(111)\n    return fig, ax\n\ndef savefig(filename, crop = True):\n    if crop == True:\n#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n        plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n    else:\n#        plt.savefig('{}.pgf'.format(filename))\n        plt.savefig('{}.pdf'.format(filename))\n        plt.savefig('{}.eps'.format(filename))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-16T13:43:08.127836Z","iopub.execute_input":"2021-12-16T13:43:08.128081Z","iopub.status.idle":"2021-12-16T13:43:08.138395Z","shell.execute_reply.started":"2021-12-16T13:43:08.128051Z","shell.execute_reply":"2021-12-16T13:43:08.137692Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sys\nimport torch\nfrom collections import OrderedDict\n\nfrom pyDOE import lhs\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nfrom scipy.interpolate import griddata\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport matplotlib.gridspec as gridspec\nimport time\n\nnp.random.seed(1234)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-16T13:43:08.139679Z","iopub.execute_input":"2021-12-16T13:43:08.140385Z","iopub.status.idle":"2021-12-16T13:43:09.963833Z","shell.execute_reply.started":"2021-12-16T13:43:08.140349Z","shell.execute_reply":"2021-12-16T13:43:09.963133Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# CUDA support \nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-16T13:43:09.965864Z","iopub.execute_input":"2021-12-16T13:43:09.966165Z","iopub.status.idle":"2021-12-16T13:43:10.010352Z","shell.execute_reply.started":"2021-12-16T13:43:09.966129Z","shell.execute_reply":"2021-12-16T13:43:10.009649Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# the deep neural network\nclass DNN(torch.nn.Module):\n    def __init__(self, layers):\n        super(DNN, self).__init__()\n        \n        # parameters\n        self.depth = len(layers) - 1\n        \n        # set up layer order dict\n        self.activation = torch.nn.Tanh\n        \n        layer_list = list()\n        for i in range(self.depth - 1): \n            layer_list.append(\n                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n            )\n            layer_list.append(('activation_%d' % i, self.activation()))\n            \n        layer_list.append(\n            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n        )\n        layerDict = OrderedDict(layer_list)\n        \n        # deploy layers\n        self.layers = torch.nn.Sequential(layerDict)\n        \n    def forward(self, x):\n        out = self.layers(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:43:10.011593Z","iopub.execute_input":"2021-12-16T13:43:10.012574Z","iopub.status.idle":"2021-12-16T13:43:10.021713Z","shell.execute_reply.started":"2021-12-16T13:43:10.012531Z","shell.execute_reply":"2021-12-16T13:43:10.021007Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"rho = 1.29\nmu = 1.1\ngx = -9.81\ngy = 0\n\nclass PhysicsInformedNN():\n    def __init__(self, X_bc1, X_bc2, X_bc3, X_f, U_bc1, layers):  \n        \n        self.v_bc = torch.tensor(U_bc1[:,0:1]).float().to(device)\n        self.p_bc = torch.tensor(U_bc1[:,1:2]).float().to(device)\n        \n        self.X_bc1_x = torch.tensor(X_bc1[:, 0:1], requires_grad=True).float().to(device)\n        self.X_bc1_y = torch.tensor(X_bc1[:, 1:2], requires_grad=True).float().to(device)\n        self.X_bc1_t = torch.tensor(X_bc1[:, 2:], requires_grad=True).float().to(device)\n        \n        self.X_bc2_x = torch.tensor(X_bc2[:, 0:1], requires_grad=True).float().to(device)\n        self.X_bc2_y = torch.tensor(X_bc2[:, 1:2], requires_grad=True).float().to(device)\n        self.X_bc2_t = torch.tensor(X_bc2[:, 2:], requires_grad=True).float().to(device)\n        \n        self.X_bc3_x = torch.tensor(X_bc3[:, 0:1], requires_grad=True).float().to(device)\n        self.X_bc3_y = torch.tensor(X_bc3[:, 1:2], requires_grad=True).float().to(device)\n        self.X_bc3_t = torch.tensor(X_bc3[:, 2:], requires_grad=True).float().to(device)\n        \n        self.X_f_x = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n        self.X_f_y = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n        self.X_f_t = torch.tensor(X_f[:, 2:], requires_grad=True).float().to(device) \n\n        self.layers = layers\n        \n        # deep neural networks\n        self.dnn = DNN(layers).to(device)\n        \n        # optimizers: using the same settings\n        self.optimizer = torch.optim.LBFGS(\n            self.dnn.parameters(), \n            lr=1.0, \n            max_iter=10000, \n            max_eval=10000, \n            history_size=50,\n            tolerance_grad=1e-5, \n            tolerance_change=1.0 * np.finfo(float).eps,\n            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n        )\n\n        self.iter = 0\n                \n    def net_u(self, x, y, t):  \n        u = self.dnn(torch.cat([x, y, t], dim=1))\n        return u\n    \n    def net_f(self, x, y, t):\n        Sol = self.net_u(x,y,t)\n        u = Sol[:,0:1]\n        v = Sol[:,1:2]\n        p = Sol[:,2:]\n        \n        u_t = torch.autograd.grad(u,t,grad_outputs=torch.ones_like(u),retain_graph=True,create_graph=True)[0]\n        v_t = torch.autograd.grad(v,t,grad_outputs=torch.ones_like(v),retain_graph=True,create_graph=True)[0]\n        \n        u_x = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(u),retain_graph=True,create_graph=True)[0]\n        v_x = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(v),retain_graph=True,create_graph=True)[0]\n        \n        u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(u_x),retain_graph=True,create_graph=True)[0]\n        v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(v_x),retain_graph=True,create_graph=True)[0]\n        \n        u_y = torch.autograd.grad(u,y,grad_outputs=torch.ones_like(u),retain_graph=True,create_graph=True)[0]\n        v_y = torch.autograd.grad(v,y,grad_outputs=torch.ones_like(v),retain_graph=True,create_graph=True)[0]\n        \n        u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(u_y),retain_graph=True,create_graph=True)[0]\n        v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(v_y),retain_graph=True,create_graph=True)[0]\n        \n        p_x = torch.autograd.grad(p,x,grad_outputs=torch.ones_like(p),retain_graph=True,create_graph=True)[0]\n        p_y = torch.autograd.grad(p,y,grad_outputs=torch.ones_like(p),retain_graph=True,create_graph=True)[0]\n        \n        f1 = rho*(u_t+ u*u_x+ v*u_y) + p_x -rho*gx - mu*(u_xx + u_yy)\n        f2 = rho*(v_t+ u*v_x+ v*v_y) + p_y -rho*gy - mu*(v_xx + v_yy)\n        return f1, f2\n    \n    def loss_func(self):\n        self.optimizer.zero_grad()\n        \n        Sol1 = self.net_u(self.X_bc1_x, self.X_bc1_y, self.X_bc1_t)\n        Sol2 = self.net_u(self.X_bc2_x, self.X_bc2_y, self.X_bc2_t)\n        Sol3 = self.net_u(self.X_bc3_x, self.X_bc3_y, self.X_bc3_t)\n        \n        loss_sol1 = torch.mean((Sol1[:,1:2]-self.v_bc)**2)\n        loss_sol1+= torch.mean((Sol1[:,2:]-self.p_bc)**2)\n        loss_sol2 = torch.mean((Sol2[:,0:1]) ** 2)\n        loss_sol3 = torch.mean((Sol3[:,0:1]) ** 2)        \n        \n        f1, f2 = self.net_f(self.X_f_x, self.X_f_y, self.X_f_t)\n        loss_f1 = torch.mean(f1**2)\n        loss_f2 = torch.mean(f2**2)\n        \n        loss = loss_sol1 + loss_sol2 + loss_sol3 + loss_f1 + loss_f2\n        \n        loss.backward()\n        self.iter += 1\n        if self.iter % 100 == 0:\n            print(\"Iter:{},Loss_sol1:{},Loss_sol2:{},Loss_sol3:{},Loss_f1:{},Loss_f2:{}\".format(self.iter, loss_sol1, loss_sol2, loss_sol3, loss_f1, loss_f2))\n        return loss        \n            \n    def train(self):\n        self.dnn.train()\n        self.optimizer.step(self.loss_func)\n            \n    def predict(self, X):\n        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n        y = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n        t = torch.tensor(X[:, 2:], requires_grad=True).float().to(device)\n\n        self.dnn.eval()\n        u = self.net_u(x,y,t)\n        f1, f2 = self.net_f(x,y,t)\n        u = u.detach().cpu().numpy()\n        f1 = f1.detach().cpu().numpy()\n        f2 = f2.detach().cpu().numpy()\n        return u, f1, f2","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:43:10.023993Z","iopub.execute_input":"2021-12-16T13:43:10.024182Z","iopub.status.idle":"2021-12-16T13:43:10.059147Z","shell.execute_reply.started":"2021-12-16T13:43:10.024159Z","shell.execute_reply":"2021-12-16T13:43:10.058302Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# the physics-guided neural network\nclass PhysicsInformedNN():\n    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n        \n        # boundary conditions\n        self.lb = torch.tensor(lb).float().to(device)\n        self.ub = torch.tensor(ub).float().to(device)\n        \n        # data\n        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n        self.u = torch.tensor(u).float().to(device)\n        \n        self.layers = layers\n        self.nu = nu\n        \n        # deep neural networks\n        self.dnn = DNN(layers).to(device)\n        \n        # optimizers: using the same settings\n        self.optimizer = torch.optim.LBFGS(\n            self.dnn.parameters(), \n            lr=1.0, \n            max_iter=50000, \n            max_eval=50000, \n            history_size=50,\n            tolerance_grad=1e-5, \n            tolerance_change=1.0 * np.finfo(float).eps,\n            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n        )\n\n        self.iter = 0\n        \n    def net_u(self, x, t):  \n        u = self.dnn(torch.cat([x, t], dim=1))\n        return u\n    \n    def net_f(self, x, t):\n        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n    \n        u = self.net_u(x, t)\n        \n        u_t = torch.autograd.grad(\n            u, t, \n            grad_outputs=torch.ones_like(u),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        u_x = torch.autograd.grad(\n            u, x, \n            grad_outputs=torch.ones_like(u),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        u_xx = torch.autograd.grad(\n            u_x, x, \n            grad_outputs=torch.ones_like(u_x),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        f = u_t + u * u_x - self.nu * u_xx\n        return f\n    \n    def loss_func(self):\n        self.optimizer.zero_grad()\n        \n        u_pred = self.net_u(self.x_u, self.t_u)\n        print(torch.mean(u_pred[:,1:])**2)\n        return\n        f_pred = self.net_f(self.x_f, self.t_f)\n        loss_u = torch.mean((self.u - u_pred) ** 2)\n        loss_f = torch.mean(f_pred ** 2)\n        \n        loss = loss_u + loss_f\n        \n        loss.backward()\n        self.iter += 1\n        if self.iter % 100 == 0:\n            print(\n                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n            )\n        return loss\n    \n    def train(self):\n        self.dnn.train()\n                \n        # Backward and optimize\n        self.optimizer.step(self.loss_func)\n\n            \n    def predict(self, X):\n        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n\n        self.dnn.eval()\n        u = self.net_u(x, t)\n        f = self.net_f(x, t)\n        u = u.detach().cpu().numpy()\n        f = f.detach().cpu().numpy()\n        return u, f","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:09:40.684303Z","iopub.execute_input":"2021-12-15T19:09:40.684625Z","iopub.status.idle":"2021-12-15T19:09:40.712424Z","shell.execute_reply.started":"2021-12-15T19:09:40.684594Z","shell.execute_reply":"2021-12-15T19:09:40.711073Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_u = 100\nN_f = 10000\nlayers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 3]\n\nt = np.array([[t/20] for t in range(10)])\nx = np.array([[t/5000] for t in range(500)])\ny = np.array([[t/1250] for t in range(-100,100)])\n\nBC1 = np.array([[0,t/1250,0] for t in range(-100,100)])\nBC2 = np.array([[a/5000,0.0792,b/200] for a in range(500) for b in range(100)])\nBC3 = np.array([[a/5000,-0.08,b/200] for a in range(500) for b in range(100)])\n\nUC1 = np.array([[0.8,0.8]]*200)\n\nlb = np.array([0.0, -0.08, 0.0])\nub = np.array([0.1, 0.08, 0.5])\nXf_train = lb + (ub-lb)*lhs(3, N_f)\n\nidx = np.random.choice(BC1.shape[0], N_u, replace=False)\nBC1 = BC1[idx, :]\nUC1 = UC1[idx,:]\n\nidx = np.random.choice(BC2.shape[0], 1000, replace=False)\nBC2 = BC2[idx, :]\nBC3 = BC3[idx, :]\n\nmodel = PhysicsInformedNN(BC1, BC2, BC3, Xf_train, UC1, layers)\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:43:10.060584Z","iopub.execute_input":"2021-12-16T13:43:10.062455Z","iopub.status.idle":"2021-12-16T13:44:11.616653Z","shell.execute_reply.started":"2021-12-16T13:43:10.062425Z","shell.execute_reply":"2021-12-16T13:44:11.615902Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X = np.array([[x/5000,y/1250,0.1] for x in range(500) for y in range(-100,100)])\nU, F1, F2 = model.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:49:38.078293Z","iopub.execute_input":"2021-12-16T13:49:38.078808Z","iopub.status.idle":"2021-12-16T13:49:38.424606Z","shell.execute_reply.started":"2021-12-16T13:49:38.078766Z","shell.execute_reply":"2021-12-16T13:49:38.423893Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"U.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:49:42.353648Z","iopub.execute_input":"2021-12-16T13:49:42.353918Z","iopub.status.idle":"2021-12-16T13:49:42.359417Z","shell.execute_reply.started":"2021-12-16T13:49:42.353889Z","shell.execute_reply":"2021-12-16T13:49:42.358767Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}